{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python-Grundlagen mit Numpy\n",
    "\n",
    "Diese Übung gibt Ihnen eine kurze Einführung in Python. Selbst wenn Sie Python schon einmal benutzt haben, wird Ihnen dies helfen, sich mit den Funktionen vertraut zu machen, die wir brauchen werden.  \n",
    "\n",
    "**Hinweise:**\n",
    "- Wir verwenden Python 3.\n",
    "- Vermeiden Sie for-Schleifen und while-Schleifen, es sei denn, Sie werden ausdrücklich dazu aufgefordert.\n",
    "- Nachdem Sie Ihre Funktion kodiert haben, führen Sie die Zelle direkt darunter aus, um zu überprüfen, ob das Ergebnis korrekt ist.\n",
    "\n",
    "**Nach dieser Aufgabe können SIe:**\n",
    "- Jupyter Notebooks verwenden\n",
    "- Numpy-Funktionen und Numpy-Matrix-/Vektoroperationen verwenden\n",
    "- das Konzept des \"Broadcasting\" verstehen\n",
    "- In der Lage sein, Code zu vektorisieren\n",
    "\n",
    "Los geht's!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [About iPython Notebooks](#0)\n",
    "    - [Exercise 1](#ex-1)\n",
    "- [1 - Building basic functions with numpy](#1)\n",
    "    - [1.1 - sigmoid function, np.exp()](#1-1)\n",
    "        - [Exercise 2 - basic_sigmoid](#ex-2)\n",
    "        - [Exercise 3 - sigmoid](#ex-3)\n",
    "    - [1.2 - Sigmoid Gradient](#1-2)\n",
    "        - [Exercise 4 - sigmoid_derivative](#ex-4)\n",
    "    - [1.3 - Reshaping arrays](#1-3)\n",
    "        - [Exercise 5 - image2vector](#ex-5)\n",
    "    - [1.4 - Normalizing rows](#1-4)\n",
    "        - [Exercise 6 - normalize_rows](#ex-6)\n",
    "        - [Exercise 7 - softmax](#ex-7)\n",
    "- [2 - Vectorization](#2)\n",
    "    - [2.1 Implement the L1 and L2 loss functions](#2-1)\n",
    "        - [Exercise 8 - L1](#ex-8)\n",
    "        - [Exercise 9 - L2](#ex-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='0'></a>\n",
    "## Über Jupyter Notebooks ##\n",
    "\n",
    "Jupyter Notebooks sind interaktive Programmierumgebungen, die in eine Webseite eingebettet sind. In diesem Kurs werden wir Jupyter Notebooks verwenden. Sie können Zellen ausführen, indem Sie entweder die Tastenkombination \"SHIFT \"+\"ENTER\" drücken oder in der oberen Leiste des Notizbuchs auf \"Zelle ausführen\" (gekennzeichnet durch ein Play-Symbol) klicken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-1'></a>\n",
    "### Aufgabe 1\n",
    "Setzen Sie test in der Zelle unten auf `\"Hello World\"`, um \"Hello World\" auszugeben, und führen Sie die beiden Zellen unten aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (≈ 1 line of code)\n",
    "# test = \n",
    "# YOUR CODE STARTS HERE\n",
    "test = \"Hello World\"\n",
    "# YOUR CODE ENDS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: Hello World\n"
     ]
    }
   ],
   "source": [
    "print (\"test: \" + test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Erwartete Ausgabe:**:\n",
    "test: Hello World"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Grundlegende Funktionen mit Numpy erstellen ##\n",
    "\n",
    "Numpy ist das wichtigste Paket für das wissenschaftliche Rechnen in Python. Es wird von einer großen Community gepflegt (www.numpy.org). In dieser Übung werden Sie mehrere wichtige Numpy-Funktionen wie `np.exp`, `np.log` und `np.reshape` kennenlernen. Sie benötigen diese Funktionen für zukünftige Aufgaben.\n",
    "\n",
    "<a name='1-1'></a>\n",
    "### 1.1 - Sigmoidfunktion, np.exp() ###\n",
    "\n",
    "Bevor Sie `np.exp()` verwenden, werden Sie zunächst `math.exp()` benutzen, um die Sigmoidfunktion zu implementieren. Sie werden dann sehen, warum `np.exp()` der Funktion `math.exp()` vorzuziehen ist.\n",
    "\n",
    "<a name='ex-2'></a>\n",
    "### Übung 2 - basic_sigmoid\n",
    "Erstellen Sie eine Funktion, die das Sigmoid einer reellen Zahl x liefert. Verwenden Sie `math.exp(x)` für die Exponentialfunktion.\n",
    "\n",
    "**Erinnerung**:\n",
    "$sigmoid(x) = \\frac{1}{1+e^{-x}}$ ist manchmal auch als logistische Funktion bekannt. Sie ist eine nichtlineare Funktion, die nicht nur im Machine Learning (Logistische Regression), sondern auch im Deep Learning verwendet wird.\n",
    "\n",
    "<img src=\"images/Sigmoid.png\" style=\"width:500px;height:228px;\">\n",
    "\n",
    "Um auf eine Funktion zu verweisen, die zu einem bestimmten Paket gehört, können Sie sie mit `Paketname.Funktion()` aufrufen. Führen Sie den folgenden Code aus, um ein Beispiel mit `math.exp()` zu sehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from public_tests import *\n",
    "\n",
    "# GRADED FUNCTION: basic_sigmoid\n",
    "\n",
    "def basic_sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute sigmoid of x.\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "    # (≈ 1 line of code)\n",
    "    # s = \n",
    "    # YOUR CODE STARTS HERE\n",
    "    s = 1 / (1 + math.exp(-x))\n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basic_sigmoid(1) = 0.7310585786300049\n",
      "\u001b[92m All tests passed.\n"
     ]
    }
   ],
   "source": [
    "print(\"basic_sigmoid(1) = \" + str(basic_sigmoid(1)))\n",
    "\n",
    "basic_sigmoid_test(basic_sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eigentlich verwenden wir die \"math\"-Bibliothek beim Deep Learning nur selten, weil die Eingaben der Funktionen reelle Zahlen sind. Beim Deep Learning verwenden wir meist Matrizen und Vektoren. Aus diesem Grund ist numpy nützlicher. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "bad operand type for unary -: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m### One reason why we use \"numpy\" instead of \"math\" in Deep Learning ###\u001b[39;00m\n\u001b[1;32m      3\u001b[0m x \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m] \u001b[38;5;66;03m# x becomes a python list object\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mbasic_sigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# you will see this give an error when you run it, because x is a vector.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 19\u001b[0m, in \u001b[0;36mbasic_sigmoid\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03mCompute sigmoid of x.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03ms -- sigmoid(x)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# (≈ 1 line of code)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# s = \u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# YOUR CODE STARTS HERE\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m math\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mx\u001b[49m))\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# YOUR CODE ENDS HERE\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m s\n",
      "\u001b[0;31mTypeError\u001b[0m: bad operand type for unary -: 'list'"
     ]
    }
   ],
   "source": [
    "### One reason why we use \"numpy\" instead of \"math\" in Deep Learning ###\n",
    "\n",
    "x = [1, 2, 3] # x becomes a python list object\n",
    "basic_sigmoid(x) # you will see this give an error when you run it, because x is a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn nämlich $ x = (x_1, x_2, ..., x_n)$ ein Zeilenvektor ist, dann wendet `np.exp(x)` die Exponentialfunktion auf jedes Element von x an. Die Ausgabe lautet also: `np.exp(x) = (e^{x_1}, e^{x_2}, ..., e^{x_n})`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.71828183  7.3890561  20.08553692]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# example of np.exp\n",
    "t_x = np.array([1, 2, 3])\n",
    "print(np.exp(t_x)) # result is (exp(1), exp(2), exp(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn x ein Vektor ist, gibt eine Python-Operation wie $s = x + 3$ oder $s = \\frac{1}{x}$ s als einen Vektor der gleichen Größe wie x aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 5 6]\n"
     ]
    }
   ],
   "source": [
    "# example of vector operation\n",
    "t_x = np.array([1, 2, 3])\n",
    "print (t_x + 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wann immer Sie weitere Informationen zu einer Numpy-Funktion benötigen, empfehlen wir Ihnen einen Blick in [die offizielle Dokumentation] (https://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.exp.html). \n",
    "\n",
    "Sie können auch eine neue Zelle im Notizbuch erstellen und `np.exp?` (zum Beispiel) schreiben, um schnellen Zugriff auf die Dokumentation zu erhalten.\n",
    "\n",
    "<a name='ex-3'></a>\n",
    "### Aufgabe 3 - sigmoid\n",
    "Implementieren Sie die Funktion sigmoid mit numpy. \n",
    "\n",
    "**Anleitung**: x kann nun entweder eine reelle Zahl, ein Vektor oder eine Matrix sein. Die Datenstrukturen, die wir in numpy verwenden, um diese Formen (Vektoren, Matrizen...) darzustellen, heißen numpy-Arrays. Mehr brauchen Sie im Moment nicht zu wissen.\n",
    "$$ \\text{Für } x \\in \\mathbb{R}^n \\text{, } sigmoid(x) = sigmoid\\begin{pmatrix}\n",
    "    x_1 \\\\\n",
    "    x_2 \\\\\n",
    "    ...  \\\\\n",
    "    x_n \\\\\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "    \\frac{1}{1+e^{-x_1}}  \\\\\n",
    "    \\frac{1}{1+e^{-x_2}}  \\\\\n",
    "    ...  \\\\\n",
    "    \\frac{1}{1+e^{-x_n}}  \\\\\n",
    "\\end{pmatrix}\\tag{1} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of x\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array of any size\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "    \n",
    "    # (≈ 1 line of code)\n",
    "    # s = \n",
    "    # YOUR CODE STARTS HERE\n",
    "    s = 1 / (1 + np.exp(-x))\n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid(t_x) = [0.73105858 0.88079708 0.95257413]\n",
      "\u001b[92m All tests passed.\n"
     ]
    }
   ],
   "source": [
    "t_x = np.array([1, 2, 3])\n",
    "print(\"sigmoid(t_x) = \" + str(sigmoid(t_x)))\n",
    "\n",
    "sigmoid_test(sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1-2'></a>\n",
    "### 1.2 - Sigmoid Gradient\n",
    "\n",
    "Wie Sie in der Vorlesung gesehen haben, müssen Sie Gradienten berechnen, um Verlustfunktionen mit Backpropagation zu optimieren. Lassen Sie uns Ihre erste Gradientenfunktion schreiben.\n",
    "\n",
    "<a name='ex-4'></a>\n",
    "### Aufgabe 4 - sigmoid_derivative\n",
    "Implementieren Sie die Funktion sigmoid_grad(), um den Gradienten der Sigmoidfunktion in Bezug auf ihre Eingabe x zu berechnen. Die Formel lautet: \n",
    "\n",
    "$$sigmoid\\_derivative(x) = \\sigma'(x) = \\sigma(x) (1 - \\sigma(x))\\tag{2}$$\n",
    "\n",
    "Diese Funktion wird häufig in zwei Schritten kodiert:\n",
    "1. Berechnen Sie s als Sigmoid von x\n",
    "2. Berechne $\\sigma'(x) = s(1-s)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(x):\n",
    "    \"\"\"\n",
    "    Compute the gradient (also called the slope or derivative) of the sigmoid function with respect to its input x.\n",
    "    You can store the output of the sigmoid function into variables and then use it to calculate the gradient.\n",
    "    \n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array\n",
    "\n",
    "    Return:\n",
    "    ds -- Your computed gradient.\n",
    "    \"\"\"\n",
    "    \n",
    "    #(≈ 2 lines of code)\n",
    "    # s = \n",
    "    # ds = \n",
    "    # YOUR CODE STARTS HERE\n",
    "    s = 1 / (1 + np.exp(-x))\n",
    "    ds = s * ( 1 - s)\n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid_derivative(t_x) = [0.19661193 0.10499359 0.04517666]\n",
      "\u001b[92m All tests passed.\n"
     ]
    }
   ],
   "source": [
    "t_x = np.array([1, 2, 3])\n",
    "print (\"sigmoid_derivative(t_x) = \" + str(sigmoid_derivative(t_x)))\n",
    "\n",
    "sigmoid_derivative_test(sigmoid_derivative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1-3'></a>\n",
    "### 1.3 - Umformen von Arrays ###\n",
    "\n",
    "Zwei häufige Numpy-Funktionen, die beim Deep Learning verwendet werden, sind [np.shape](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.shape.html) und [np.reshape()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html). \n",
    "- X.shape wird verwendet, um die Form (Dimension) einer Matrix/eines Vektors X zu erhalten. \n",
    "- X.reshape(...) wird verwendet, um X in eine andere Dimension umzuformen. \n",
    "\n",
    "In der Informatik wird zum Beispiel ein Bild durch ein 3D-Array der Form $(Länge, Höhe, Tiefe = 3)$ dargestellt. Wenn Sie jedoch ein Bild als Eingabe für einen Algorithmus lesen, wandeln Sie es in einen Vektor der Form $(Länge*Höhe*3, 1)$ um. Mit anderen Worten, man \"entrollt\" oder formt das 3D-Array in einen 1D-Vektor um.\n",
    "\n",
    "<img src=\"images/image2vector_kiank.png\" style=\"width:500px;height:300;\">\n",
    "\n",
    "<a name='ex-5'></a>\n",
    "### Aufgabe 5 - image2vector\n",
    "Implementieren Sie `image2vector()`, das eine Eingabe der Form (Länge, Höhe, 3) annimmt und einen Vektor der Form (Länge\\*Höhe\\*3, 1) zurückgibt. Wenn Sie zum Beispiel ein Array v der Form (a, b, c) in einen Vektor der Form (a*b,c) umformen möchten, würden Sie dies wie folgt tun:\n",
    "```` python\n",
    "v = v.reshape((v.shape[0] * v.shape[1], v.shape[2])) # v.form[0] = a ; v.form[1] = b ; v.form[2] = c\n",
    "````\n",
    "\n",
    "- Bitte geben Sie die Abmessungen des Bildes nicht als Konstante ein. Schauen Sie stattdessen mit `image.shape[0]` nach, welche Größen Sie benötigen, usw. \n",
    "- Sie können v = v.reshape(-1, 1) verwenden. Stellen Sie nur sicher, dass Sie verstehen, warum es funktioniert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image2vector(image):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    image -- a numpy array of shape (length, height, depth)\n",
    "    \n",
    "    Returns:\n",
    "    v -- a vector of shape (length*height*depth, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # (≈ 1 line of code)\n",
    "    # v =\n",
    "    # YOUR CODE STARTS HERE\n",
    "    v = image.reshape((image.shape[2] * image.shape[1] * image.shape[0], 1))\n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image2vector(image) = [[0.67826139]\n",
      " [0.29380381]\n",
      " [0.90714982]\n",
      " [0.52835647]\n",
      " [0.4215251 ]\n",
      " [0.45017551]\n",
      " [0.92814219]\n",
      " [0.96677647]\n",
      " [0.85304703]\n",
      " [0.52351845]\n",
      " [0.19981397]\n",
      " [0.27417313]\n",
      " [0.60659855]\n",
      " [0.00533165]\n",
      " [0.10820313]\n",
      " [0.49978937]\n",
      " [0.34144279]\n",
      " [0.94630077]]\n",
      "\u001b[92m All tests passed.\n"
     ]
    }
   ],
   "source": [
    "# This is a 3 by 3 by 2 array, typically images will be (num_px_x, num_px_y,3) where 3 represents the RGB values\n",
    "t_image = np.array([[[ 0.67826139,  0.29380381],\n",
    "                     [ 0.90714982,  0.52835647],\n",
    "                     [ 0.4215251 ,  0.45017551]],\n",
    "\n",
    "                   [[ 0.92814219,  0.96677647],\n",
    "                    [ 0.85304703,  0.52351845],\n",
    "                    [ 0.19981397,  0.27417313]],\n",
    "\n",
    "                   [[ 0.60659855,  0.00533165],\n",
    "                    [ 0.10820313,  0.49978937],\n",
    "                    [ 0.34144279,  0.94630077]]])\n",
    "\n",
    "print (\"image2vector(image) = \" + str(image2vector(t_image)))\n",
    "\n",
    "image2vector_test(image2vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1-4'></a>\n",
    "### 1.4 - Zeilen normalisieren\n",
    "\n",
    "Eine weitere gängige Technik, die wir beim maschinellen Lernen und Deep Learning verwenden, ist die Normalisierung unserer Daten. Dies führt oft zu einer besseren Leistung, da der Gradientenabstieg nach der Normalisierung schneller konvergiert. Unter Normalisierung verstehen wir hier die Änderung von x in $ \\frac{x}{\\| x\\|} $ (Division jedes Zeilenvektors von x durch seine Norm).\n",
    "\n",
    "Zum Beispiel, wenn \n",
    "$$x = \\begin{bmatrix}\n",
    "        0 & 3 & 4 \\\\\n",
    "        2 & 6 & 4 \\\\\n",
    "\\end{bmatrix}\\tag{3}$$ \n",
    "dann \n",
    "$$\\| x\\| = \\text{np.linalg.norm(x, axis=1, keepdims=True)} = \\begin{bmatrix}\n",
    "    5 \\\\\n",
    "    \\sqrt{56} \\\\\n",
    "\\end{bmatrix}\\tag{4} $$\n",
    "und\n",
    "$$ x\\_normalisiert = \\frac{x}{\\| x\\|} = \\begin{bmatrix}\n",
    "    0 & \\frac{3}{5} & \\frac{4}{5} \\\\\n",
    "    \\frac{2}{\\sqrt{56}} & \\frac{6}{\\sqrt{56}} & \\frac{4}{\\sqrt{56}} \\\\\n",
    "\\end{bmatrix}\\tag{5}$$ \n",
    "\n",
    "Beachten Sie, dass Sie Matrizen unterschiedlicher Größe teilen können und es funktioniert einwandfrei: Das nennt man Broadcasting und Sie werden es später kennenlernen.\n",
    "\n",
    "Mit `keepdims=True` wird das Ergebnis korrekt auf das ursprüngliche x übertragen.\n",
    "\n",
    "`Achse=1` bedeutet, dass Sie die Norm zeilenweise erhalten. Wenn Sie die Norm spaltenweise benötigen, müssen Sie `axis=0` setzen. \n",
    "\n",
    "numpy.linalg.norm hat einen weiteren Parameter `ord`, mit dem die Art der Normalisierung angegeben wird (in der folgenden Übung werden Sie die 2-Norm verwenden). Um sich mit den Arten der Normalisierung vertraut zu machen, können Sie [numpy.linalg.norm](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html) besuchen.\n",
    "\n",
    "<a name='ex-6'></a>\n",
    "### Aufgabe 6 - normalize_rows\n",
    "Implementieren Sie normalize_rows(), um die Zeilen einer Matrix zu normalisieren. Nach Anwendung dieser Funktion auf eine Eingabematrix x sollte jede Zeile von x ein Vektor mit Einheitslänge (d.h. Länge 1) sein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_rows(x):\n",
    "    \"\"\"\n",
    "    Implement a function that normalizes each row of the matrix x (to have unit length).\n",
    "    \n",
    "    Argument:\n",
    "    x -- A numpy matrix of shape (n, m)\n",
    "    \n",
    "    Returns:\n",
    "    x -- The normalized (by row) numpy matrix. You are allowed to modify x.\n",
    "    \"\"\"\n",
    "    \n",
    "    #(≈ 2 lines of code)\n",
    "    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n",
    "    # x_norm =\n",
    "    # Divide x by its norm.\n",
    "    # x =\n",
    "    # YOUR CODE STARTS HERE\n",
    "    x_norm = np.linalg.norm(x,ord = 2,axis=1,keepdims=True)    \n",
    "    x = x / x_norm\n",
    "    # YOUR CODE ENDS HERE\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalizeRows(x) = [[0.         0.6        0.8       ]\n",
      " [0.13736056 0.82416338 0.54944226]]\n",
      "\u001b[92m All tests passed.\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[0., 3., 4.],\n",
    "              [1., 6., 4.]])\n",
    "print(\"normalizeRows(x) = \" + str(normalize_rows(x)))\n",
    "\n",
    "normalizeRows_test(normalize_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hinweis**:\n",
    "In normalize_rows() können Sie versuchen, die Dimensionen / Shapes von x_norm und x auszugeben, und dann die Berechnung erneut durchführen. Sie werden feststellen, dass sie unterschiedliche Dimensionen / Shapes haben. Das ist normal, denn x_norm nimmt die Norm jeder Zeile von x. x_norm hat also die gleiche Anzahl von Zeilen, aber nur 1 Spalte. Wie funktionierte es also, wenn man x durch x_norm teilte? Das nennt man Broadcasting (siehe unten)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-7'></a>\n",
    "### Übung 7 - softmax\n",
    "Implementieren Sie nun eine softmax-Funktion mit numpy. Sie können sich softmax als eine Normalisierungsfunktion vorstellen, die verwendet wird, wenn Ihr Algorithmus zwei oder mehr Klassen klassifizieren muss. Sie werden im Laufe der Vorlesung mehr über softmax lernen.\n",
    "\n",
    "**Anweisungen**:\n",
    "- $\\text{für} x \\in \\mathbb{R}^{1\\times n} \\text{, }$\n",
    "\n",
    "\\begin{align*}\n",
    " softmax(x) &= softmax\\left(\\begin{bmatrix}\n",
    "    x_1  &&\n",
    "    x_2 &&\n",
    "    ...  &&\n",
    "    x_n  \n",
    "\\end{bmatrix}\\right) \\\\&= \\begin{bmatrix}\n",
    "    \\frac{e^{x_1}}{\\sum_{j}e^{x_j}}  &&\n",
    "    \\frac{e^{x_2}}{\\sum_{j}e^{x_j}}  &&\n",
    "    ...  &&\n",
    "    \\frac{e^{x_n}}{\\sum_{j}e^{x_j}} \n",
    "\\end{bmatrix} \n",
    "\\end{align*}\n",
    "\n",
    "- $\\text{für eine Matrix } x \\in \\mathbb{R}^{m \\times n} \\text{ bildet } x_{ij} \\text{ das Element in der } i. \\text{ Zeile und } j. \\text{ Spalte von x ab, also haben wir:} $  \n",
    "\n",
    "\\begin{align*}\n",
    "softmax(x) &= softmax\\begin{bmatrix}\n",
    "            x_{11} & x_{12} & x_{13} & \\dots  & x_{1n} \\\\\n",
    "            x_{21} & x_{22} & x_{23} & \\dots  & x_{2n} \\\\\n",
    "            \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "            x_{m1} & x_{m2} & x_{m3} & \\dots  & x_{mn}\n",
    "            \\end{bmatrix} \\\\ \\\\&= \n",
    " \\begin{bmatrix}\n",
    "    \\frac{e^{x_{11}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{12}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{13}}}{\\sum_{j}e^{x_{1j}}} & \\dots  & \\frac{e^{x_{1n}}}{\\sum_{j}e^{x_{1j}}} \\\\\n",
    "    \\frac{e^{x_{21}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{22}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{23}}}{\\sum_{j}e^{x_{2j}}} & \\dots  & \\frac{e^{x_{2n}}}{\\sum_{j}e^{x_{2j}}} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\frac{e^{x_{m1}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m2}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m3}}}{\\sum_{j}e^{x_{mj}}} & \\dots  & \\frac{e^{x_{mn}}}{\\sum_{j}e^{x_{mj}}}\n",
    "\\end{bmatrix} \\\\ \\\\ &= \\begin{pmatrix}\n",
    "    softmax\\text{(erste Reihe von x)}  \\\\\n",
    "    softmax\\text{(zweite Reihe von x)} \\\\\n",
    "    \\vdots  \\\\\n",
    "    softmax\\text{(letzte Reihe von x)} \\\\\n",
    "\\end{pmatrix} \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Anmerkungen:**\n",
    "Beachten Sie, dass im weiteren Verlauf der Vorlesung \"m\" für die \"Anzahl der Trainingsbeispiele\" verwendet wird und jedes Trainingsbeispiel in einer eigenen Spalte der Matrix steht. Außerdem befindet sich jedes Merkmal in einer eigenen Zeile (jede Zeile enthält Daten für dasselbe Merkmal).  \n",
    "\n",
    "In dieser Programmierpraxis geht es jedoch nur darum, sich mit Python vertraut zu machen, daher verwenden wir die übliche mathematische Notation $m \\times n$  \n",
    "wobei $m$ für die Anzahl der Zeilen und $n$ für die Anzahl der Spalten steht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Calculates the softmax for each row of the input x.\n",
    "\n",
    "    Your code should work for a row vector and also for matrices of shape (m,n).\n",
    "\n",
    "    Argument:\n",
    "    x -- A numpy matrix of shape (m,n)\n",
    "\n",
    "    Returns:\n",
    "    s -- A numpy matrix equal to the softmax of x, of shape (m,n)\n",
    "    \"\"\"\n",
    "    \n",
    "    #(≈ 3 lines of code)\n",
    "    # Apply exp() element-wise to x. Use np.exp(...).\n",
    "    # x_exp = ...\n",
    "\n",
    "    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n",
    "    # x_sum = ...\n",
    "    \n",
    "    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n",
    "    # s = ...\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    x_exp = np.exp(x)\n",
    "    x_sum = np.sum(x_exp, axis = 1, keepdims = True)\n",
    "    s = x_exp / x_sum\n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax(x) = [[9.80897665e-01 8.94462891e-04 1.79657674e-02 1.21052389e-04\n",
      "  1.21052389e-04]\n",
      " [8.78679856e-01 1.18916387e-01 8.01252314e-04 8.01252314e-04\n",
      "  8.01252314e-04]]\n",
      "\u001b[92m All tests passed.\n"
     ]
    }
   ],
   "source": [
    "t_x = np.array([[9, 2, 5, 0, 0],\n",
    "                [7, 5, 0, 0 ,0]])\n",
    "print(\"softmax(x) = \" + str(softmax(t_x)))\n",
    "\n",
    "softmax_test(softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hinweise\n",
    "- Wenn Sie die Formen von x_exp, x_sum und s oben ausgeben und die Bewertungszelle erneut ausführen, werden Sie sehen, dass x_sum die Form (2,1) hat, während x_exp und s die Form (2,5) haben. **x_exp/x_sum** funktioniert aufgrund desBroadcastings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Vektorisierung\n",
    "\n",
    "\n",
    "Beim Deep Learning hat man es mit sehr großen Datensätzen zu tun. Daher kann eine rechnerisch nicht optimale Funktion zu einem großen Engpass in Ihrem Algorithmus werden und zu einem Modell führen, das ewig braucht, um zu trainieren. Um sicherzustellen, dass Ihr Code rechnerisch effizient ist, werden Sie Vektorisierung verwenden. Versuchen Sie zum Beispiel, den Unterschied zwischen den folgenden Implementierungen des Skalarprodukts zu erkennen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot = 278\n",
      " ----- Computation time = 0.04212900000011288ms\n",
      "outer = [[81. 18. 18. 81.  0. 81. 18. 45.  0.  0. 81. 18. 45.  0.  0.]\n",
      " [18.  4.  4. 18.  0. 18.  4. 10.  0.  0. 18.  4. 10.  0.  0.]\n",
      " [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [63. 14. 14. 63.  0. 63. 14. 35.  0.  0. 63. 14. 35.  0.  0.]\n",
      " [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [81. 18. 18. 81.  0. 81. 18. 45.  0.  0. 81. 18. 45.  0.  0.]\n",
      " [18.  4.  4. 18.  0. 18.  4. 10.  0.  0. 18.  4. 10.  0.  0.]\n",
      " [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      " ----- Computation time = 0.08020000000019678ms\n",
      "elementwise multiplication = [81.  4. 10.  0.  0. 63. 10.  0.  0.  0. 81.  4. 25.  0.  0.]\n",
      " ----- Computation time = 0.043581999999986465ms\n",
      "gdot = [20.43849356 29.46984277 12.14214848]\n",
      " ----- Computation time = 0.06425099999995965ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\n",
    "x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n",
    "\n",
    "### CLASSIC DOT PRODUCT OF VECTORS IMPLEMENTATION ###\n",
    "tic = time.process_time()\n",
    "dot = 0\n",
    "\n",
    "for i in range(len(x1)):\n",
    "    dot += x1[i] * x2[i]\n",
    "toc = time.process_time()\n",
    "print (\"dot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000 * (toc - tic)) + \"ms\")\n",
    "\n",
    "### CLASSIC OUTER PRODUCT IMPLEMENTATION ###\n",
    "tic = time.process_time()\n",
    "outer = np.zeros((len(x1), len(x2))) # we create a len(x1)*len(x2) matrix with only zeros\n",
    "\n",
    "for i in range(len(x1)):\n",
    "    for j in range(len(x2)):\n",
    "        outer[i,j] = x1[i] * x2[j]\n",
    "toc = time.process_time()\n",
    "print (\"outer = \" + str(outer) + \"\\n ----- Computation time = \" + str(1000 * (toc - tic)) + \"ms\")\n",
    "\n",
    "### CLASSIC ELEMENTWISE IMPLEMENTATION ###\n",
    "tic = time.process_time()\n",
    "mul = np.zeros(len(x1))\n",
    "\n",
    "for i in range(len(x1)):\n",
    "    mul[i] = x1[i] * x2[i]\n",
    "toc = time.process_time()\n",
    "print (\"elementwise multiplication = \" + str(mul) + \"\\n ----- Computation time = \" + str(1000 * (toc - tic)) + \"ms\")\n",
    "\n",
    "### CLASSIC GENERAL DOT PRODUCT IMPLEMENTATION ###\n",
    "W = np.random.rand(3,len(x1)) # Random 3*len(x1) numpy array\n",
    "tic = time.process_time()\n",
    "gdot = np.zeros(W.shape[0])\n",
    "\n",
    "for i in range(W.shape[0]):\n",
    "    for j in range(len(x1)):\n",
    "        gdot[i] += W[i,j] * x1[j]\n",
    "toc = time.process_time()\n",
    "print (\"gdot = \" + str(gdot) + \"\\n ----- Computation time = \" + str(1000 * (toc - tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot = 278\n",
      " ----- Computation time = 0.04644799999997673ms\n",
      "outer = [[81 18 18 81  0 81 18 45  0  0 81 18 45  0  0]\n",
      " [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0]\n",
      " [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [63 14 14 63  0 63 14 35  0  0 63 14 35  0  0]\n",
      " [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [81 18 18 81  0 81 18 45  0  0 81 18 45  0  0]\n",
      " [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0]\n",
      " [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
      " ----- Computation time = 0.04361199999980414ms\n",
      "elementwise multiplication = [81  4 10  0  0 63 10  0  0  0 81  4 25  0  0]\n",
      " ----- Computation time = 0.023083999999951033ms\n",
      "gdot = [20.43849356 29.46984277 12.14214848]\n",
      " ----- Computation time = 0.11761099999985092ms\n"
     ]
    }
   ],
   "source": [
    "x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\n",
    "x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n",
    "\n",
    "### VECTORIZED DOT PRODUCT OF VECTORS ###\n",
    "tic = time.process_time()\n",
    "dot = np.dot(x1,x2)\n",
    "toc = time.process_time()\n",
    "print (\"dot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000 * (toc - tic)) + \"ms\")\n",
    "\n",
    "### VECTORIZED OUTER PRODUCT ###\n",
    "tic = time.process_time()\n",
    "outer = np.outer(x1,x2)\n",
    "toc = time.process_time()\n",
    "print (\"outer = \" + str(outer) + \"\\n ----- Computation time = \" + str(1000 * (toc - tic)) + \"ms\")\n",
    "\n",
    "### VECTORIZED ELEMENTWISE MULTIPLICATION ###\n",
    "tic = time.process_time()\n",
    "mul = np.multiply(x1,x2)\n",
    "toc = time.process_time()\n",
    "print (\"elementwise multiplication = \" + str(mul) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### VECTORIZED GENERAL DOT PRODUCT ###\n",
    "tic = time.process_time()\n",
    "dot = np.dot(W,x1)\n",
    "toc = time.process_time()\n",
    "print (\"gdot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000 * (toc - tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie Sie vielleicht bemerkt haben, ist die vektorisierte Implementierung viel sauberer und effizienter. Bei größeren Vektoren/Matrizen werden die Unterschiede in der Laufzeit noch größer. \n",
    "\n",
    "**Anmerkung**: `np.dot()` führt eine Matrix-Matrix- oder Matrix-Vektor-Multiplikation durch. Dies unterscheidet sich von `np.multiply()` und dem `*`-Operator (der äquivalent zu `.*` in Matlab/Octave ist), der eine elementweise Multiplikation durchführt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2-1'></a>\n",
    "### 2.1 Implementierung der Verlustfunktionen L1 und L2\n",
    "\n",
    "<a name='ex-8'></a>\n",
    "### Aufgabe 8 - L1 \n",
    "Implementieren Sie die numpy-vektorisierte Version der L1-Verlustfunktion. Vielleicht finden Sie die Funktion abs(x) (absoluter Wert von x) dabei nützlich.\n",
    "\n",
    "**Erinnerung**:\n",
    "- Die Verlustfunktion wird verwendet, um die Leistung Ihres Modells zu bewerten. Je größer der Verlust ist, desto mehr weichen Ihre Vorhersagen ($ \\hat{y} $) von den wahren Werten ($y$) ab. Beim Deep Learning verwenden Sie Optimierungsalgorithmen wie Gradient Descent, um Ihr Modell zu trainieren und die Kosten zu minimieren.\n",
    "- Der L1-Verlust ist definiert als:\n",
    "$$\\begin{align*} &\\mathscr{L_1}(\\hat{y}, y) = \\sum_{i=0}^{m-1}|y^{(i)} - \\hat{y}^{(i)}| \\end{align*}\\tag{6}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def L1(yhat, y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    yhat -- vector of size m (predicted labels)\n",
    "    y -- vector of size m (true labels)\n",
    "    \n",
    "    Returns:\n",
    "    loss -- the value of the L1 loss function defined above\n",
    "    \"\"\"\n",
    "    \n",
    "    #(≈ 1 line of code)\n",
    "    # loss = \n",
    "    # YOUR CODE STARTS HERE\n",
    "    loss = sum(abs(y-yhat))\n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 = 1.1\n",
      "\u001b[92m All tests passed.\n"
     ]
    }
   ],
   "source": [
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "print(\"L1 = \" + str(L1(yhat, y)))\n",
    "\n",
    "L1_test(L1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-9'></a>\n",
    "### Aufgabe 9 - L2\n",
    "Implementieren Sie die numpy-vektorisierte Version des L2-Verlustes. Es gibt mehrere Möglichkeiten, den L2-Verlust zu implementieren, aber vielleicht ist die Funktion np.dot() nützlich. Zur Erinnerung: Wenn $x = [x_1, x_2, ..., x_n]$, dann ist `np.dot(x,x)` = $\\sum_{j=1}^n x_j^{2}$. \n",
    "\n",
    "- L2-Verlust ist definiert als $$\\begin{align*} & \\mathscr{L_2}(\\hat{y},y) = \\sum_{i=0}^{m-1}(y^{(i)} - \\hat{y}^{(i)})^2 \\end{align*}\\tag{7}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L2(yhat, y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    yhat -- vector of size m (predicted labels)\n",
    "    y -- vector of size m (true labels)\n",
    "    \n",
    "    Returns:\n",
    "    loss -- the value of the L2 loss function defined above\n",
    "    \"\"\"\n",
    "    \n",
    "    #(≈ 1 line of code)\n",
    "    # loss = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    loss = sum((y-yhat)**2)\n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 = 0.43\n",
      "\u001b[92m All tests passed.\n"
     ]
    }
   ],
   "source": [
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "\n",
    "print(\"L2 = \" + str(L2(yhat, y)))\n",
    "\n",
    "L2_test(L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Herzlichen Glückwunsch zur Bewältigung dieser Übung. Wir hoffen, dass diese kleine Aufwärmübung Ihnen bei den zukünftigen Aufgaben hilft."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Fazit:</b>\n",
    "    \n",
    "- Die Vektorisierung ist beim Deep Learning sehr wichtig. Sie sorgt für Berechnungseffizienz und Klarheit.\n",
    "- Sie haben den L1- und L2-Verlust kennengelernt.\n",
    "- Sie sind nun mit vielen Numpy-Funktionen wie np.sum, np.dot, np.multiply, np.maximum usw. vertraut."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
