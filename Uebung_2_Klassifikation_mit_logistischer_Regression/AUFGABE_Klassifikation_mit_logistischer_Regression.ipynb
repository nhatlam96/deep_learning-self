{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Klassifikation mit logistischer Regression\n",
    "\n",
    "**Ziel**: Wir implementieren einen einfachen Klassifizierer basierend auf logistischer Regression. Das System erhält Bilder als Input und soll klassifizieren, ob darauf eine Katze zu sehen ist oder nicht.**\n",
    "\n",
    "**Anweisungen:**\n",
    "- Verwenden Sie keine Schleifen (for/while) in Ihrem Code, es sei denn, die Anweisungen fordern Sie ausdrücklich dazu auf.\n",
    "- Verwenden Sie `np.dot(X,Y)`, um das Skalarprodukt zweier Vektoren zu berechnen.\n",
    "\n",
    "**Quelle:** DeepLearning.AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schritte\n",
    "- [1 - Bibliotheken](#1)\n",
    "- [2 - Problemstellung](#2)\n",
    "    - [Aufgabe 1 1](#ex-1)\n",
    "    - [Aufgabe 2 2](#ex-2)\n",
    "- [3 - Allgemeine Architektur eines Lernalgorithmus](#3)\n",
    "- [4 - Aufbau der Teile unseres Algorithmus](#4)\n",
    "    - [4.1 - Hilfsfunktionen](#4-1)\n",
    "        - [Aufgabe 3 - sigmoid](#ex-3)\n",
    "    - [4.2 - Initialisierung der Parameter](#4-2)\n",
    "        - [Aufgabe 4 - Initialisierung mit Nullen](#ex-4)\n",
    "    - [4.3 - Forward and Backward Propagation](#4-3)\n",
    "        - [Aufgabe 5 - propagate](#ex-5)\n",
    "    - [4.4 - Optimierung](#4-4)\n",
    "        - [Aufgabe 6 - Optimieren](#ex-6)\n",
    "        - [Aufgabe 7 - Vorhersage / Prediction](#ex-7)\n",
    "- [5 - Alle Funktionen zu einem Modell zusammenfügen](#5)\n",
    "    - [Aufgabe 8 - Modell](#ex-8)\n",
    "- [6 - Weitere Analyse](#6)\n",
    "- [7 - Testen Sie mit Ihrem eigenen Bild](#7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Bibliotheken ##\n",
    "\n",
    "Führen Sie zunächst die unten stehende Zelle aus, um alle Pakete zu importieren, die Sie für diese Aufgabe benötigen. \n",
    "- [numpy](https://numpy.org/doc/1.20/) ist das grundlegende Paket für mehrdimensionale Datenstrukturen und wissenschaftliche Berechnungen mit Python.\n",
    "- [h5py](http://www.h5py.org) ist ein allgemeines Paket zur Interaktion mit einem Datensatz, der in einer [H5-Datei](https://de.wikipedia.org/wiki/Hierarchical_Data_Format) gespeichert ist.\n",
    "- [matplotlib](http://matplotlib.org) ist eine Bibliothek zum Zeichnen von Diagrammen in Python.\n",
    "- [PIL](https://pillow.readthedocs.io/en/stable/) und [scipy](https://www.scipy.org/) werden  verwendet, um Ihr Modell mit einem eigenen Bild am Ende zu testen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from lr_utils import load_dataset\n",
    "from public_tests import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Problemstellung ##\n",
    "\n",
    "Sie erhalten einen Datensatz (\"data.h5\"), der enthält:\n",
    "    - eine Trainingsmenge von m_train Bildern, die als Katze (y=1) oder Nicht-Katze (y=0) gelabelt sind\n",
    "    - einen Testsatz von m_test Bildern, die als Katze oder Nicht-Katze gekennzeichnet sind\n",
    "    - jedes Bild hat die Form (num_px, num_px, 3), wobei 3 für die 3 Kanäle (RGB) steht. Jedes Bild ist also quadratisch (Höhe = num_px) und (Breite = num_px).\n",
    "\n",
    "Sie werden einen einfachen Algorithmus erstellen, der Bilder als Katze oder Nicht-Katze klassifiziert.\n",
    "\n",
    "Machen wir uns zunächst mit dem Datensatz vertraut. Laden Sie die Daten, indem Sie den folgenden Code ausführen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data (cat/non-cat)\n",
    "train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir haben \"_orig\" am Ende der Bilddatensätze (train und test) hinzugefügt, weil wir sie vorverarbeiten werden. Nach der Vorverarbeitung erhalten wir train_set_x und test_set_x (die Labels train_set_y und test_set_y müssen nicht vorverarbeitet werden).\n",
    "\n",
    "Jede Zeile von train_set_x_orig und test_set_x_orig ist ein Array, das ein Bild darstellt. Sie können sich ein Beispiel ansehen, indem Sie den folgenden Code ausführen. Sie können auch den `index- Wert ändern und erneut ausführen, um andere Bilder zu sehen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = [1], it's a 'cat' picture.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of a picture\n",
    "index = 25\n",
    "plt.imshow(train_set_x_orig[index])\n",
    "print(\n",
    "    f\"y = {str(train_set_y[:, index])}, it's a '\"\n",
    "    + classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\")\n",
    "    + \"' picture.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viele Softwarefehler beim Deep Learning entstehen durch nicht passende Matrix-/Vektordimensionen. Wenn Sie Ihre Matrix-/Vektor-Dimensionen im Griff halten, werden Sie viele Fehler vermeiden. \n",
    "\n",
    "<a name='ex-1'></a>\n",
    "### Aufgabe 1\n",
    "Finden Sie die Werte für:\n",
    "    - m_train (Anzahl der Trainingsbeispiele)\n",
    "    - m_test (Anzahl der Testbeispiele)\n",
    "    - num_px (= Höhe = Breite eines Trainingsbildes)\n",
    "Denken Sie daran, dass `train_set_x_orig` ein numpy-Array der Form (m_train, num_px, num_px, 3) ist. Sie können zum Beispiel auf `m_train` zugreifen, indem Sie `train_set_x_orig.shape[0]` schreiben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(≈ 3 lines of code)\n",
    "# m_train = \n",
    "# m_test = \n",
    "# num_px = \n",
    "# YOUR CODE STARTS HERE\n",
    "\n",
    "\n",
    "# YOUR CODE ENDS HERE\n",
    "\n",
    "print (\"Number of training examples: m_train = \" + str(m_train))\n",
    "print (\"Number of testing examples: m_test = \" + str(m_test))\n",
    "print (\"Height/Width of each image: num_px = \" + str(num_px))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print (\"train_set_x shape: \" + str(train_set_x_orig.shape))\n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print (\"test_set_x shape: \" + str(test_set_x_orig.shape))\n",
    "print (\"test_set_y shape: \" + str(test_set_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Erwartete Ausgabe für m_train, m_test and num_px**: \n",
    "<table style=\"width:15%\">\n",
    "  <tr>\n",
    "    <td> m_train </td>\n",
    "    <td> 209 </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>m_test</td>\n",
    "    <td> 50 </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>num_px</td>\n",
    "    <td> 64 </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Einfachheit halber sollten Sie nun Bilder der Form (num_px, num_px, 3) in ein numpy-Array der Form (num_px $*$ num_px $*$ 3, 1) umformen. Danach ist unser Trainings- (und Test-) Datensatz ein numpy-array, in dem jede Spalte ein reduziertes Bild darstellt. Es sollten m_train (bzw. m_test) Spalten sein.\n",
    "\n",
    "<a name='ex-2'></a>\n",
    "### Aufgabe 2\n",
    "Formen Sie die Trainings- und Testdatensätze so um, dass Bilder der Größe (num_px, num_px, 3) in einzelne Vektoren der Form (num\\_px $*$ num\\_px $*$ 3, 1) \"flachgeklopft\"\" werden.\n",
    "\n",
    "Ein Trick, wenn man eine Matrix X der Form (a,b,c,d) in eine Matrix X_flatten der Form (b$*$c$*$d, a) umwandeln will, ist die Verwendung von: \n",
    "```python\n",
    "X_flatten = X.reshape(X.shape[0], -1).T # X.T ist die Transponierte von X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the training and test examples\n",
    "#(≈ 2 lines of code)\n",
    "# train_set_x_flatten = ...\n",
    "# test_set_x_flatten = ...\n",
    "# YOUR CODE STARTS HERE\n",
    "\n",
    "\n",
    "# YOUR CODE ENDS HERE\n",
    "\n",
    "# Check that the first 10 pixels of the second image are in the correct place\n",
    "assert np.alltrue(train_set_x_flatten[0:10, 1] == [196, 192, 190, 193, 186, 182, 188, 179, 174, 213]), \"Wrong solution. Use (X.shape[0], -1).T.\"\n",
    "assert np.alltrue(test_set_x_flatten[0:10, 1] == [115, 110, 111, 137, 129, 129, 155, 146, 145, 159]), \"Wrong solution. Use (X.shape[0], -1).T.\"\n",
    "\n",
    "print (\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape))\n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print (\"test_set_x_flatten shape: \" + str(test_set_x_flatten.shape))\n",
    "print (\"test_set_y shape: \" + str(test_set_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Erwartete Ausgabe:**: \n",
    "\n",
    "<table style=\"width:35%\">\n",
    "  <tr>\n",
    "    <td>train_set_x_flatten shape</td>\n",
    "    <td> (12288, 209)</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>train_set_y shape</td>\n",
    "    <td>(1, 209)</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>test_set_x_flatten shape</td>\n",
    "    <td>(12288, 50)</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>test_set_y shape</td>\n",
    "    <td>(1, 50)</td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zur Darstellung von Farbbildern müssen die Rot-, Grün- und Blaukanäle (RGB) für jedes Pixel angegeben werden, so dass der Pixelwert eigentlich ein Vektor aus drei Zahlen zwischen 0 und 255 ist.\n",
    "\n",
    "Ein üblicher Vorverarbeitungsschritt beim maschinellen Lernen besteht darin, den Datensatz zu zentrieren und zu standardisieren. Das bedeutet, dass Sie den Mittelwert des gesamten Numpy-Arrays von jedem Beispiel abziehen und dann jedes Beispiel durch die Standardabweichung des gesamten Numpy-Arrays dividieren. Bei Bilddatensätzen ist es jedoch einfacher und bequemer und funktioniert fast genauso gut, wenn Sie einfach jede Zeile des Datensatzes durch 255 (den Höchstwert eines Pixelkanals) teilen.\n",
    "\n",
    "<!-- Während des Trainings Ihres Modells werden Sie Parameter mit initialen Eingabewerten multiplizieren Dann trainieren Sie das Modell durch Backpropagation mit den Gradienten. Es ist jedoch äußerst wichtig, dass jedes Feature einen ähnlichen Wertebereich hat, damit unsere Gradienten nicht explodieren.!--> \n",
    "\n",
    "Lassen Sie nun unseren Datensatz standardisieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x = train_set_x_flatten / 255.\n",
    "test_set_x = test_set_x_flatten / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Was Sie sich merken müssen:**\n",
    "\n",
    "Die üblichen Schritte zur Vorverarbeitung eines neuen Datensatzes sind:\n",
    "- Bestimmen Sie die Anzahl der Dimensionen und die Shapes des Problems (m_train, m_test, num_px, ...).\n",
    "- Standardisieren Sie die Daten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - Allgemeine Architektur eines Lernalgorithmus ##\n",
    "\n",
    "Wir werden jetzt einen einfachen Algorithmus entwickeln, um Katzenbilder von Nicht-Katzenbildern zu unterscheiden.\n",
    "\n",
    "Sie werden eine logistische Regression entwickeln. Die folgende Abbildung erklärt, dass die logistische Regression eigentlich ein sehr einfaches Neuronales Netz ist!\n",
    "\n",
    "<img src=\"images/LogReg_kiank.png\" style=\"width:650px;height:400px;\">\n",
    "\n",
    "**Mathematische Darstellung des Algorithmus**:\n",
    "\n",
    "Für ein Trainingsbeispiel: $x^{(i)}$:\n",
    "$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$\n",
    "$$\\hat{y}^{(i)} = a^{(i)} = \\sigma(z^{(i)})\\tag{2}$$ \n",
    "$$ \\mathscr{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}$$\n",
    "\n",
    "Die Kosten werden dann als Summe über alle Trainingsdaten berechnet::\n",
    "$$ \\mathscr{J} = \\frac{1}{m} \\sum_{i=1}^m \\mathscr{L}(a^{(i)}, y^{(i)})\\tag{6}$$\n",
    "\n",
    "**Schritte**:\n",
    "In dieser Übung führen Sie die folgenden Schritte durch: \n",
    "- Initialisieren Sie die Parameter des Modells\n",
    "- Lernen Sie die Parameter des Modells durch Minimierung der Kostenfunktion  \n",
    "- Verwenden Sie die erlernten Parameter, um Predictions zu berechnen (auf dem Testdatensatz)\n",
    "- Analysieren Sie die Ergebnisse und ziehen Sie Schlussfolgerungen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4 - Aufbau der Teile unseres Algorithmus ## \n",
    "\n",
    "Die wichtigsten Schritte zum Aufbau eines neuronalen Netzes sind:\n",
    "1. Definieren der Modellstruktur (z.B. Anzahl der Eingangsmerkmale / Features) \n",
    "2. Initialisierung der Parameter des Modells\n",
    "3. Schleife:\n",
    "    - Berechnung des aktuellen Verlustwerts, d.h. der Abweichung zwischen vorhergesagten und tatsächlichen Werten (Vorwärtspropagation)\n",
    "    - Berechnen des aktuellen Gradienten (Rückwärtspropagation)\n",
    "    - Aktualisierung der Parameter (Gradientenabstieg / Gradient Descent)\n",
    "\n",
    "Oft baut man die SChritte 1-3 separat auf und integriert sie in eine Funktion, die wir `Modell()` nennen.\n",
    "\n",
    "<a name='4-1'></a>\n",
    "### 4.1 - Hilfsfunktionen\n",
    "\n",
    "<a name='ex-3'></a>\n",
    "### Aufgabe 3 - sigmoid\n",
    "Implementieren Sie `sigmoid()`. Wie Sie in der Abbildung oben gesehen haben, müssen Sie $sigmoid(z) = \\sigma(z) = \\frac{1}{1 + e^{-z}}$ für $z = w^T x + b$ berechnen, um Vorhersagen zu machen. Verwenden Sie np.exp()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "\n",
    "    #(≈ 1 line of code)\n",
    "    # s = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"sigmoid([0, 2]) = \" + str(sigmoid(np.array([0,2]))))\n",
    "\n",
    "sigmoid_test(sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([0.5, 0, 2.0])\n",
    "output = sigmoid(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4-2'></a>\n",
    "### 4.2 - Initialisierung der Parameter\n",
    "\n",
    "<a name='ex-4'></a>\n",
    "### Aufgabe 4 - Initialisierung mit Nullen\n",
    "Implementieren Sie die Initialisierung der Parameter in der folgenden Zelle. Sie müssen w als einen Vektor von Nullen initialisieren. Wenn Sie nicht wissen, welche Numpy-Funktion Sie verwenden sollen, schlagen Sie in der Dokumentation der Numpy-Bibliothek unter np.zeros() nach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_with_zeros(dim):\n",
    "    \"\"\"\n",
    "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
    "    \n",
    "    Argument:\n",
    "    dim -- size of the w vector we want (or number of parameters in this case)\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim, 1)\n",
    "    b -- initialized scalar (corresponds to the bias) of type float\n",
    "    \"\"\"\n",
    "    \n",
    "    # (≈ 2 lines of code)\n",
    "    # w = ...\n",
    "    # b = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 2\n",
    "w, b = initialize_with_zeros(dim)\n",
    "\n",
    "assert type(b) == float\n",
    "print (\"w = \" + str(w))\n",
    "print (\"b = \" + str(b))\n",
    "\n",
    "initialize_with_zeros_test_1(initialize_with_zeros)\n",
    "initialize_with_zeros_test_2(initialize_with_zeros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4-3'></a>\n",
    "### 4.3 - Forward and Backward Propagation\n",
    "\n",
    "Nun, da die Parameter initialisiert sind, können Sie die Schritte \"Forward and Backward Propagation\" zum Lernen der Parameter durchführen.\n",
    "\n",
    "<a name='ex-5'></a>\n",
    "### Aufgabe 5 - propagate\n",
    "Implementieren Sie eine Funktion `propagate()`, die die Kostenfunktion und ihren Gradienten berechnet.\n",
    "\n",
    "**Hinweise**:\n",
    "\n",
    "Forward Propagation:\n",
    "- Eingabe: X\n",
    "- Sie berechnen: $A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$\n",
    "- Sie berechnen die Kostenfunktion: $\\mathscr{J} = -\\frac{1}{m}\\sum_{i=1}^{m}(y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)}))$\n",
    "\n",
    "Zwei Formeln, die Sie verwenden werden: \n",
    "\n",
    "$$ \\frac{\\partial \\mathscr{J}}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}$$\n",
    "$$ \\frac{\\partial \\mathscr{J}}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function and its gradient for the propagation explained above\n",
    "\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n",
    "\n",
    "    Return:\n",
    "    grads -- dictionary containing the gradients of the weights and bias\n",
    "            (dw -- gradient of the loss with respect to w, thus same shape as w)\n",
    "            (db -- gradient of the loss with respect to b, thus same shape as b)\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    \n",
    "    Tips:\n",
    "    - Write your code step by step for the propagation. np.log(), np.dot()\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # FORWARD PROPAGATION (FROM X TO COST)\n",
    "    #(≈ 2 lines of code)\n",
    "    # compute activation\n",
    "    # A = ...\n",
    "    # compute cost by using np.dot to perform multiplication. \n",
    "    # And don't use loops for the sum.\n",
    "    # cost = ...                                \n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "\n",
    "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
    "    #(≈ 2 lines of code)\n",
    "    # dw = ...\n",
    "    # db = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    cost = np.squeeze(np.array(cost))\n",
    "\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w =  np.array([[1.], [2]])\n",
    "b = 1.5\n",
    "\n",
    "# X is using 3 examples, with 2 features each\n",
    "# Each example is stacked column-wise\n",
    "X = np.array([[1., -2., -1.], [3., 0.5, -3.2]])\n",
    "Y = np.array([[1, 1, 0]])\n",
    "grads, cost = propagate(w, b, X, Y)\n",
    "\n",
    "assert type(grads[\"dw\"]) == np.ndarray\n",
    "assert grads[\"dw\"].shape == (2, 1)\n",
    "assert type(grads[\"db\"]) == np.float64\n",
    "\n",
    "\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))\n",
    "print (\"cost = \" + str(cost))\n",
    "\n",
    "propagate_test(propagate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Erwartete Ausgabe:**\n",
    "\n",
    "```\n",
    "dw = [[ 0.25071532]\n",
    " [-0.06604096]]\n",
    "db = -0.1250040450043965\n",
    "cost = 0.15900537707692405"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4-4'></a>\n",
    "### 4.4 - Optimierung\n",
    "- Sie haben Ihre Parameter initialisiert.\n",
    "- Sie sind auch in der Lage, eine Kostenfunktion und ihren Gradienten zu berechnen.\n",
    "- Nun wollen Sie die Parameter mit Hilfe des Gradientenabstiegs / Gradient Descent aktualisieren.\n",
    "\n",
    "<a name='ex-6'></a>\n",
    "### Aufgabe 6 - Optimieren\n",
    "Schreiben Sie die Optimierungsfunktion. Ziel ist es, $w$ und $b$ durch Minimierung der Kostenfunktion $\\mathscr{J}$ zu lernen. Für einen Parameter $\\theta$ lautet die Aktualisierungsregel $ \\theta = \\theta - \\alpha \\text{ } d\\theta$, wobei $\\alpha$ die Lernrate ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(w, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    Tips:\n",
    "    You basically need to write down two steps and iterate through them:\n",
    "        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n",
    "        2) Update the parameters using gradient descent rule for w and b.\n",
    "    \"\"\"\n",
    "    \n",
    "    w = copy.deepcopy(w)\n",
    "    b = copy.deepcopy(b)\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # (≈ 1 lines of code)\n",
    "        # Cost and gradient calculation \n",
    "        # grads, cost = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "        \n",
    "        # Retrieve derivatives from grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # update rule (≈ 2 lines of code)\n",
    "        # w = ...\n",
    "        # b = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "        \n",
    "        # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "            # Print the cost every 100 training iterations\n",
    "            if print_cost:\n",
    "                print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params, grads, costs = optimize(w, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False)\n",
    "\n",
    "print (\"w = \" + str(params[\"w\"]))\n",
    "print (\"b = \" + str(params[\"b\"]))\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))\n",
    "print(\"Costs = \" + str(costs))\n",
    "\n",
    "optimize_test(optimize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-7'></a>\n",
    "### Aufgabe 7 - Vorhersage / Prediction\n",
    "Die vorherige Funktion gibt die gelernten w und b aus. Wir können w und b verwenden, um die Labels für einen Datensatz X vorherzusagen. Implementieren Sie die Funktion `predict()`. Es gibt zwei Schritte zur Berechnung von Vorhersagen:\n",
    "\n",
    "1. Berechnen Sie $\\hat{Y} = A = \\sigma(w^T X + b)$\n",
    "\n",
    "2. Konvertiere die Einträge von a in 0 (wenn Aktivierung <= 0,5) oder 1 (wenn Aktivierung > 0,5) und speichere die Vorhersagen in einem Vektor `Y_prediction`. Wenn Sie möchten, können Sie eine `if`/`else`-Anweisung in einer `for`-Schleife verwenden (es gibt jedoch auch eine Möglichkeit, dies zu vektorisieren). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1, m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n",
    "    #(≈ 1 line of code)\n",
    "    # A = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        \n",
    "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
    "        #(≈ 4 lines of code)\n",
    "        # if A[0, i] > ____ :\n",
    "        #     Y_prediction[0,i] = \n",
    "        # else:\n",
    "        #     Y_prediction[0,i] = \n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([[0.1124579], [0.23106775]])\n",
    "b = -0.3\n",
    "X = np.array([[1., -1.1, -3.2],[1.2, 2., 0.1]])\n",
    "print (\"predictions = \" + str(predict(w, b, X)))\n",
    "\n",
    "predict_test(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Was Sie sich merken sollten:**\n",
    "    \n",
    "Sie haben mehrere Funktionen implementiert, die folgendes bewirken:\n",
    "- Initialisierung (w,b)\n",
    "- Iterative Optimierung des Verlusts, um die Parameter (w,b) zu lernen:\n",
    "  - Berechnen der Kosten und ihres Gradienten \n",
    "  - Aktualisierung der Parameter durch Gradientenabstieg / Gradient Descent\n",
    "- Verwendung der erlernten (w,b) zur Vorhersage der Bezeichnungen für eine gegebene Menge von Beispielen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "## 5 - Alle Funktionen zu einem Modell zusammenfügen ##\n",
    "\n",
    "Sie werden nun sehen, wie das Gesamtmodell aufgebaut ist, indem Sie alle Bausteine (Funktionen, die in den vorherigen Teilen implementiert wurden) in der richtigen Reihenfolge zusammenfügen.\n",
    "\n",
    "<a name='ex-8'></a>\n",
    "### Aufgabe 8 - Modell\n",
    "Implementieren Sie die Modellfunktion. Verwenden Sie die folgende Notation:\n",
    "- Y_prediction_test für Ihre Vorhersagen auf der Testmenge\n",
    "- Y_prediction_train für Ihre Vorhersagen für die Trainingsmenge\n",
    "- parameters, grads, costs für die Ausgaben von optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5, print_cost=False):\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the function you've implemented previously\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
    "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
    "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
    "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
    "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to True to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "    # (≈ 1 line of code)   \n",
    "    # initialize parameters with zeros\n",
    "    # and use the \"shape\" function to get the first dimension of X_train\n",
    "    # w, b = ...\n",
    "    \n",
    "    #(≈ 1 line of code)\n",
    "    # Gradient descent \n",
    "    # params, grads, costs = ...\n",
    "    \n",
    "    # Retrieve parameters w and b from dictionary \"params\"\n",
    "    # w = ...\n",
    "    # b = ...\n",
    "    \n",
    "    # Predict test/train set examples (≈ 2 lines of code)\n",
    "    # Y_prediction_test = ...\n",
    "    # Y_prediction_train = ...\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "\n",
    "    # Print train/test Errors\n",
    "    if print_cost:\n",
    "        print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "        print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from public_tests import *\n",
    "\n",
    "model_test(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn alle Tests durchlaufen, führen Sie die folgende Zelle aus, um Ihr Modell zu trainieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_model = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations=2000, learning_rate=0.005, print_cost=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bemerkung**: Die Trainingsgenauigkeit liegt nahe bei 100%. Dies ist eine guter Check: Ihr Modell funktioniert und hat eine ausreichende Kapazität, um die Trainingsdaten zu erfassen. Die Testgenauigkeit beträgt 70%. Das ist eigentlich nicht schlecht für dieses einfache Modell, wenn man bedenkt, dass wir einen kleinen Datensatz verwendet haben und dass die logistische Regression ein einfacher, linearer Klassifikator ist. (Wir werden dies noch verbessern.)\n",
    "\n",
    "Sie sehen auch, dass das Modell die Trainingsdaten eindeutig \"overfittet\". Overfitting ist ein Problem beim Trainieren von Machine-Learning-Modellen, das auftritt, wenn ein Modell zu komplex ist und zu viele Datenmerkmale (Features) im Trainingsdatensatz lernt, einschließlich des Rauschens und der Ausreißer. Das Modell wird so spezifisch für den Trainingsdatensatz, dass seine Leistung auf neuen, unbekannten Daten deutlich schlechter ist. Es hat im Wesentlichen die zufälligen Fluktuationen in den Trainingsdaten „auswendig gelernt“, statt die zugrundeliegenden Muster zu erfassen, die auf neue Daten generalisiert werden können. Später werden Sie lernen, wie Sie das Overfitting reduzieren können, z. B. durch den Einsatz von Regularisierung. Mit dem folgenden Code (und der Änderung der Variable `index`) können Sie sich die Vorhersagen für die Bilder der Testmenge ansehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a picture that was wrongly classified.\n",
    "index = 1\n",
    "plt.imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n",
    "print (\"y = \" + str(test_set_y[0,index]) + \", you predicted that it is a \\\"\" + classes[int(logistic_regression_model['Y_prediction_test'][0,index])].decode(\"utf-8\") +  \"\\\" picture.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir zeichnen die Kostenfunktion und die Gradienten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curve (with costs)\n",
    "costs = np.squeeze(logistic_regression_model['costs'])\n",
    "plt.plot(costs)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per hundreds)')\n",
    "plt.title(\"Learning rate =\" + str(logistic_regression_model[\"learning_rate\"]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**:\n",
    "Sie können sehen, dass die Kosten sinken. Das zeigt, dass die Parameter gelernt werden. Sie sehen jedoch, dass Sie das Modell noch weiter auf dem Trainingssatz trainieren könnten. Versuchen Sie, die Anzahl der Iterationen in der Zelle oben zu erhöhen und führen Sie die Zellen erneut aus. Sie werden feststellen, dass die Genauigkeit der Trainingsmenge steigt, die Genauigkeit der Testmenge jedoch sinkt. Dies zeigt das Overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6'></a>\n",
    "## 6 - Weitere Analyse ##\n",
    "\n",
    "Herzlichen Glückwunsch zur Erstellung Ihres ersten Bildklassifikationsmodells. Analysieren wir es weiter und untersuchen mögliche Wahloptionen für die Lernrate $\\alpha$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wahl der Lernrate ####\n",
    "\n",
    "**Erinnerung**:\n",
    "Damit der Gradientenabstieg funktioniert, müssen Sie die Lernrate sinnvoll wählen. Die Lernrate $\\alpha$ bestimmt, wie schnell wir die Parameter aktualisieren. Wenn die Lernrate zu groß ist, kann es passieren, dass wir über den optimalen Wert hinausschießen. Wenn sie zu klein ist, brauchen wir zu viele Iterationen, um zu den besten Werten zu konvergieren. Deshalb ist es so wichtig, eine gut abgestimmte Lernrate zu verwenden.\n",
    "\n",
    "Lassen Sie uns die Lernkurve unseres Modells bei verschiedenen Lernraten vergleichen. Führen Sie die unten stehende Zelle aus. Dies sollte etwa 1 Minute dauern. Probieren Sie ruhig auch andere Werte aus als die drei, die wir in die Variable `learning_rates` eingestellt haben, und sehen Sie, was passiert. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "models = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print (\"Training a model with learning rate: \" + str(lr))\n",
    "    models[str(lr)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations=1500, learning_rate=lr, print_cost=False)\n",
    "    print ('\\n' + \"-------------------------------------------------------\" + '\\n')\n",
    "\n",
    "for lr in learning_rates:\n",
    "    plt.plot(np.squeeze(models[str(lr)][\"costs\"]), label=str(models[str(lr)][\"learning_rate\"]))\n",
    "\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (hundreds)')\n",
    "\n",
    "legend = plt.legend(loc='upper center', shadow=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**: \n",
    "- Unterschiedliche Lernraten führen zu unterschiedlichen Kosten und damit zu unterschiedlichen Vorhersageergebnissen.\n",
    "- Wenn die Lernrate zu groß ist (0,01), können die Kosten nach oben und unten schwanken. Sie können sogar divergieren (obwohl in diesem Beispiel die Verwendung von 0,01 immer noch zu einem guten Wert für die Kosten führt). \n",
    "- Niedrigere Kosten bedeuten nicht gleich ein besseres Modell. Sie müssen prüfen, ob möglicherweise eine Überanpassung / ein Overfitting vorliegt. Dies ist der Fall, wenn die Trainingsgenauigkeit viel höher ist als die Testgenauigkeit.\n",
    "- Beim Deep Learning sollten Sie wie folgt vorgehen: \n",
    "    - Wählen Sie eine Lernrate, welche die Kostenfunktion am besten minimiert.\n",
    "    - Wenn Ihr Modell overfittet, verwenden Sie weitere Techniken, um das Overfitting zu verringern. (Wir werden diese später behandeln.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='7'></a>\n",
    "## 7 - Testen Sie mit Ihrem eigenen Bild ##\n",
    "\n",
    "Herzlichen Glückwunsch zum Abschluss dieser Übung. Sie können nun ein eigenes Bild verwenden und die Ergebnisse Ihres Modells betrachten:\n",
    "1. Fügen Sie Ihr Bild in das Verzeichnis dieses Jupyter-Notizbuchs im Ordner \"images\" ein.\n",
    "2. Fügen Sie den Dateinamen Ihres Bildes in folgendem Code ein.\n",
    "3. Führen Sie den Code aus und überprüfen Sie, ob der Algorithmus eine korrekte Vorhersage macht (1 = Katze, 0 = nicht Katze)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bibliography:\n",
    "- http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/\n",
    "- https://stats.stackexchange.com/questions/211436/why-do-we-normalize-images-by-subtracting-the-datasets-image-mean-and-not-the-c\n",
    "\n",
    "Quelle: DeepLearning.AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this to the name of your image file\n",
    "my_image = \"my_image.jpg\"   \n",
    "\n",
    "# We preprocess the image to fit your algorithm.\n",
    "fname = \"images/\" + my_image\n",
    "image = np.array(Image.open(fname).resize((num_px, num_px)))\n",
    "plt.imshow(image)\n",
    "image = image / 255.\n",
    "image = image.reshape((1, num_px * num_px * 3)).T\n",
    "my_predicted_image = predict(logistic_regression_model[\"w\"], logistic_regression_model[\"b\"], image)\n",
    "\n",
    "print(\"y = \" + str(np.squeeze(my_predicted_image)) + \", your algorithm predicts a \\\"\" + classes[int(np.squeeze(my_predicted_image)),].decode(\"utf-8\") +  \"\\\" picture.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Was Sie sich merken sollten:**\n",
    "1. Die Vorverarbeitung des Datensatzes ist wichtig.\n",
    "2. Sie haben jede Funktion separat implementiert: initialize(), propagate(), optimize(). Dann haben Sie ein Modell() aus den einzelnen Verarbeitungsschritten erstellt.\n",
    "3. Die Abstimmung der Lernrate (ein Beispiel für einen \"Hyperparameter\") kann einen großen Unterschied für die Effektivität des Algorithmus ausmachen. Wir werden später weitere Beispiele dafür sehen!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimentieren Sie gerne weiter\n",
    "    - Spielen Sie mit der Lernrate und der Anzahl der Iterationen\n",
    "    - Probieren Sie verschiedene Initialisierungsmethoden aus und vergleichen Sie die Ergebnisse\n",
    "    - Testen Sie andere Vorverarbeitungsschritte (zentrieren Sie die Daten, oder teilen Sie jede Zeile durch ihre Standardabweichung)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
